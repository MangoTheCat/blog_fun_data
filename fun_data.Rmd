---
title: 'Fun data: open data that is fun to analyse'
output: 
  html_document:
    keep_md: true
---

```{r setup, include=FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, out.width='900px', dpi=200)
library(tidyverse)
```

Jeremy Singer-Vine sends out a [newsletter](https://tinyletter.com/data-is-plural) every week where he highlights a number of interesting open datasets (you can explore all the datasets [here](https://docs.google.com/spreadsheets/d/1wZhPLMCHKJvwOkP4juclhjFgqIY8fQFMemwKL2c64vk/edit#gid=0)). At Mango we are all for open data so we thought we would also share some of the open datasets we think are fun to explore.  

### Open Food Facts
[Open Food Facts](https://world.openfoodfacts.org/) is a collaborative, free and open database of food products. It is a prime example of how effective crowdsourcing your data is. People from around the world have collected details about more than 300.000 food products and uploaded the information through mobile apps. The data is available as a MongoDB dump, CSV export and an experimental API. We have downloaded the CSV export and wil try to visualise the ingredients across all products.

```{r, eval=FALSE, collapse=TRUE, include=FALSE}
# http://world.openfoodfacts.org/data/en.openfoodfacts.org.products.csv
foodFacts <- readr::read_tsv("data/en.openfoodfacts.org.products.csv")
dim(foodFacts)
```
```{r, eval=FALSE, include=FALSE}
ingredients <- foodFacts %>%
  # ideally, the unnest_tokens function is what we want but it was too slow
  #tidytext::unnest_tokens(ingredient, ingredients_text,
  #                        token = stringr::str_split, pattern=",|\\(|\\)|\\[|\\]") %>%
  # so instead we chose a different approach involving transmute and unlist
  # transmute will give us a list-column
  transmute(ingredients = stringr::str_split(ingredients_text, pattern=",|\\(|\\)|\\[|\\]")) %>%
  # unlist will convert the list-column into a character vector
  unlist() %>%
  # enframe will convert the vector into a data frame which is easier to groupby
  enframe() %>%
  # now we clean up some of the text
  mutate(value = stringr::str_replace(value, "org|organic", ""),
         value = stringr::str_replace(value, "-> en:\\w+", ""),
         value = stringr::str_replace_all(value, "'", ""),
         value = stringr::str_trim(value)) %>%
  # and finally filter some of the weirder entries
  filter(value!="", value!=".",
         !stringr::str_detect(value, "completed|\\d")) %>%
  # to then group by and count the ingredients
  count(value) %>%
  arrange(desc(n))
head(ingredients, 10)
```

There are no surprises at the top but further down there are a few ingredients that are odd. Let's create a word cloud to show the relative frequencies.

```{r, eval=FALSE, include=FALSE}
library(wordcloud)
top100 <- head(ingredients, 100)
wordcloud::wordcloud(top100$value, top100$n)
```

Ingredients are only one aspect of this very interesting dataset. We could go on and look at the co-occurence of certain ingredients (using network analysis) and then continue analysing their quantities. We could also include the data on nutritional value and calculate correlations. The data could also use some more cleaning considering there some ingredients in different languages (e.g. water and eau).

### North Korea Missile Attacks
```{r, eval=FALSE, include=FALSE}

```



### Food prices
Following in the edible theme, [VAM](http://vam.wfp.org/) collate commodity prices from the globes poorer nations and use them in helping to
identify food insecurity hotspots. The data we will be using can be downloaded [here](https://data.humdata.org/dataset/wfp-food-prices),
from which we'll attempt to visulaise how prices have changed over the past 20 years.

```{r, eval=TRUE, collapse=TRUE, include=TRUE}
# https://data.humdata.org/dataset/wfp-food-prices
#Providing column names and types to make workings easier later on
colnames <- c("Country_ID", "Country", "Region_ID", "Region", "Market_ID", "Market", 
              "Commodity_ID", "Commodity", "Currency_ID", "Currency", "Sector_ID", "Sector",
              "Units_ID", "Units", "Month", "Year", "Price", "Commodity_Source")
coltypes <- cols(Year = "n", Price = "n", Month = "n")
foodPrices <- read_csv("data/FoodPrices/WFPVAM_FoodPrices_13-03-2017.csv", 
                       col_names = colnames,
                       col_types = coltypes) %>%
  filter(row_number() != 1)

#Large number of commodities - won't be able to plot them all!
length(unique(foodPrices$Commodity))
```

```{r, eval=TRUE, include=TRUE}
#Overall price trend - trend of all commodity prices over time
overallPriceTrend <- foodPrices %>%
  group_by(Commodity, Year) %>%
  #As different commodities will clearly have different prices, we'll make
  #them comparable by scaling based on their max price within our timeframe
  summarise(globalAveragePrice = mean(Price)/max(Price))

#Food prices trend over time, grouped by commodity - same as above, 
#this time selecting a smaller sample for plotting
commodoties <- c("Wheat", "Milk", "Coffee", "Bananas", "Sugar")
priceTrend <- foodPrices %>%
  select(Price, Commodity, Year) %>%
  #selecting our reduced commodities
  filter(Commodity %in% commodoties) %>%
  group_by(Commodity, Year) %>%
  summarise(globalAveragePrice = mean(Price)/max(Price))

priceTrend
```
We can see what our data looks like for each commodity.

Now lets create a graphic.
```{r, eval=TRUE, include=TRUE}
library(forcats)
#We'll create a ggplot graphic, using geom_smooth
#Specify some colours semi-related to their genuine appearence
colours <- c("Bananas" = "#fea814", "Coffee" = "#383838", "Sugar" = "#4fcfff", 
             "Milk" = "#cccccc", "Wheat" = "#005692")
#Specify commodity levels for use in our legend
fctLevels <- c("Coffee", "Bananas", "Milk", "Sugar", "Wheat")
ggplot(priceTrend, aes(x = Year, y = globalAveragePrice)) +
  geom_smooth(aes(colour = fct_relevel(Commodity, fctLevels)), se = FALSE, size = 1.8, linetype = 5) +
  geom_smooth(data = overallPriceTrend, colour = "red", se = FALSE, size = 3.5) +
  geom_line(aes(size = "All Commodities", linetype = NA), colour = "red") +
  scale_colour_manual(values = colours) +
  scale_x_continuous(breaks = seq(1992, 2017, 2)) +
  scale_y_continuous(limits = c(0, 1), labels = c("Min", "Max"), breaks = c(0, 1)) +
  labs(title = "Average Global Commodity Prices over Time",
       subtitle = "Commodity Price Relative to Max in Period",
       caption = "Data from https://data.humdata.org/dataset/wfp-food-prices",
       x = "", 
       y = "",
       colour = "Commodity",
       size = "") +
  theme_classic()
```

The trend for our selected commodities seems to show a gradual decrease in prices, as all but coffee and milk prices are now lower than
at the beginning of our timeframe. This is somewhat reflected in our overall price trend, as we can see there has been a slight downward trend, although this has been somewhat negated in the past three years.

In our analysis we took only a meer peek into the dataset. For example, we could look at seasonality trends, subset by country or region or even by market. Indeed it could be taken a step further, as VAM have, and be used as a tool for predicting when and where food security will
occur in the future.


### Flight Delays
The [Bureau of Transportation Statistics](https://www.bts.gov/) are a leading source of U.S. transportation systems data, helping
to shape transportation policy and researh projects across the US. I pulled through their [data](https://www.transtats.bts.gov/DL_SelectFields.asp), selecting the Month and DayOfWeek fields, as well as Departure and Arrival delays. The site only allows us to download data one month at a time, so we need to begin by reading in 12 files to a list and binding
them together.

```{r, eval=TRUE, collapse=TRUE, include=TRUE}
# https://www.transtats.bts.gov/DL_SelectFields.asp
#Month files were downloaded to separate csvs, so we'll read them into list
files <- list.files(path = "data/FlightData/", pattern = "*.csv", full.names = TRUE)
myFiles <- lapply(files, read_csv)

#And then join into one dataframe using bind_rows
flightsRaw <- do.call(bind_rows, myFiles)
```

Now lets recode our Months and Weekdays to be character names, rather than just integers. Then we can prepare our data for plotting. In
this plot, we'll be aiming to plot the difference between departure and arrival delays, grouped by Weekday and Month.
```{r, eval=TRUE, include=TRUE}
#Begin by renaming integer values to days of the week and months
flightData <- flightsRaw %>%
  mutate(DAY_OF_WEEK = as.factor(DAY_OF_WEEK),
         DAY_OF_WEEK = fct_recode(DAY_OF_WEEK, Monday = "1", Tuesday = "2", Wenesday = "3",
                              Thursday = "4", Friday = "5", Saturday = "6", Sunday = "7"),
         MONTH = as.factor(MONTH),
         MONTH = fct_recode(MONTH, January = "1", February = "2", March = "3", April = "4",
                            May = "5", June = "6", July = "7", August = "8", September = "9",
                            October = "10", November = "11", December = "12"))

#Summarise the data to show the mean delay/arrival time split by Month and Weekday
monthDayDelays <- flightData %>%
  select(DEP_DELAY, ARR_DELAY, MONTH, DAY_OF_WEEK) %>%
  group_by(MONTH, DAY_OF_WEEK) %>%
  #Calculate means split by Month and Weekday
  summarise(Departures = mean(DEP_DELAY, na.rm = TRUE), Arrivals = mean(ARR_DELAY, na.rm = TRUE)) %>%
  filter(!is.na(MONTH)) %>%
  ungroup() %>%
  #Gather our data so Depature and Arrival delay times are stored within one column
  gather(key = delayType, value = Delay, Arrivals, Departures) %>%
  #Reorder Departure/Arrival factors so Depatures appear first in our graphic
  mutate(delayType = fct_rev(delayType))
monthDayDelays
```
We now have our Delay times in one coloumn, with delayType either arrival or departure. We also have our Month and Weekday columns for 
grouping our data in the plot.

Now lets create a graphic. Note, the colours used for our graphic were obtained from [Color Brewer 2.0](http://colorbrewer2.org).
```{r, eval=TRUE, include=TRUE}
#Specify colours vector for use in scale_fill_manual.
colours <- c("#ffffb2", "#fed976", "#feb24c", "#fd8d3c", "#fc4e2a", "#e31a1c", "#b10026")
#Graphic
ggplot(monthDayDelays, aes(x = MONTH, y = Delay)) +
  #Adding reference lines to our (soon to become) circular chart
  geom_hline(yintercept = seq(0, 20, by = 5), alpha = 0.1) +
  #Bars for every day of the week grouped by month
  geom_bar(stat = "identity", aes(fill = DAY_OF_WEEK), position = "dodge", colour = "black") +
  #This bit is hacky. To get a single label to appear on our reference lines at zero degrees, we select a
  #single datapoint from January (Month at zero degrees) and use it as our data argument.
  #monthDayDelays[1, ] is a datapoint for mean Arrival time in January 
  geom_text(data = monthDayDelays[1, ], y = 5, label = "5", size = 3) +
  geom_text(data = monthDayDelays[1, ], y = 10, label = "10", size = 3) +
  geom_text(data = monthDayDelays[1, ], y = 15, label = "15", size = 3) +
  geom_text(data = monthDayDelays[1, ], y = 20, label = "20", size = 3) +
  #monthDayDelays[85, ] is a datapoint for mean Departure time in January 
  geom_text(data = monthDayDelays[85, ], y = 10, label = "10", size = 3) +
  geom_text(data = monthDayDelays[85, ], y = 15, label = "15", size = 3) +
  geom_text(data = monthDayDelays[85, ], y = 20, label = "20", size = 3) +
  #Specify colours for chart
  scale_fill_manual(values = rev(colours)) + 
  #Remove yAxis scale from side of plot
  scale_y_continuous(breaks = NULL) +
  #Make plot circular, starting at due north
  coord_polar(theta = "x", start = -pi/12) + 
  #Separate our departure and arrival plots
  facet_wrap(~delayType, labeller = label_parsed) +
  #Add labels
  labs(title = "Average Minute Delay of American Flights - 2016",
       x = "",
       y = "", 
       fill = "",
       caption = "Data from https://www.transtats.bts.gov/DL_SelectFields.asp") +
  theme_minimal(base_size = 8) +
  #Make our facet titles pretty
  theme(strip.text.x = element_text(size = 15, colour = "#f03b20"))
```

Interestingly, we can see that even if your flight is delayed, you can expect that delay to have reduced upon arrival. In fact,
on average 5 minutes and 25 seconds will be recovered during air time.

Perhaps not so interestingly, we can see that the months with the longest delays fall in the summer and winter holidays. Conversely,
if you're not bound by school holidays, November looks to be an excellent time to travel, as you can expect to arrive a whole
2 minutes early!

Again with this analysis, there are many more pathways we could explore. For example, [FiveThirtyEight](https://projects.fivethirtyeight.com/flights/) have produced a similar delay chart, only this time grouped by airport. This, when combined with our brief analysis, makes your dream, undelayed, vacation a possible reality!!